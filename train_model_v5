import json
from collections import Counter
from transformers import BertTokenizerFast, BertForTokenClassification
from torch.utils.data import DataLoader, TensorDataset
import torch
from torch.nn import CrossEntropyLoss
from tqdm import tqdm
from seqeval.metrics import classification_report
import os

# Vérifiez si un GPU est disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Charger les fichiers
def load_jsonlines(filepath):
    with open(filepath, 'r') as f:
        return [json.loads(line) for line in f]

training_data = load_jsonlines('C:/Users/adrie/OneDrive/Bureau/IPSA/Aero5/Ma513/Projet/Ma513_Project/Data/NER-TRAINING.jsonlines')
validation_data = load_jsonlines('C:/Users/adrie/OneDrive/Bureau/IPSA/Aero5/Ma513/Projet/Ma513_Project/Data/NER-VALIDATION.jsonlines')

# Définir les étiquettes uniques
all_tags = [tag for entry in training_data for tag in entry['ner_tags']]
unique_labels = list(set(all_tags))
print("Unique labels:", unique_labels)

# Calculer des poids pour les classes
tag_counts = Counter(all_tags)
total_tags = sum(tag_counts.values())
class_weights = [total_tags / tag_counts[label] for label in unique_labels]
class_weights = torch.tensor(class_weights).to(device)

# Prétraitement des données
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')

def encode_data(data, tokenizer, unique_labels, max_length=256):
    input_ids, attention_masks, label_ids = [], [], []
    label2id = {label: idx for idx, label in enumerate(unique_labels)}

    for entry in data:
        tokens = entry['tokens']
        tags = entry.get('ner_tags', [])
        
        encodings = tokenizer(tokens, is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)
        input_ids.append(encodings['input_ids'])
        attention_masks.append(encodings['attention_mask'])
        
        if tags:
            encoded_tags = [label2id.get(tag, -100) for tag in tags]
            padded_tags = encoded_tags[:max_length] + [-100] * (max_length - len(encoded_tags))
            label_ids.append(padded_tags)
        else:
            label_ids.append([-100] * max_length)
    
    return input_ids, attention_masks, label_ids

train_inputs, train_masks, train_labels = encode_data(training_data, tokenizer, unique_labels)
val_inputs, val_masks, val_labels = encode_data(validation_data, tokenizer, unique_labels)

def create_dataloader(input_ids, attention_masks, label_ids, batch_size=16):
    inputs = torch.tensor(input_ids)
    masks = torch.tensor(attention_masks)
    labels = torch.tensor(label_ids)
    dataset = TensorDataset(inputs, masks, labels)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_loader = create_dataloader(train_inputs, train_masks, train_labels)
val_loader = create_dataloader(val_inputs, val_masks, val_labels)

# Charger et entraîner le modèle
model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
loss_fn = CrossEntropyLoss(weight=class_weights)

# Entraînement avec validation
epochs = 50  # Augmenter si nécessaire pour maximiser les performances
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    model.train()
    total_loss = 0

    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch + 1}")
    for i, batch in progress_bar:
        input_ids, attention_masks, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        
        progress_bar.set_postfix(loss=loss.item())

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}")

    # Validation après chaque époque
    print(f"Validating Epoch {epoch + 1}/{epochs}...")
    model.eval()
    true_labels, pred_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids, attention_masks, labels = [b.to(device) for b in batch]
            outputs = model(input_ids, attention_mask=attention_masks)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            for i, label_seq in enumerate(labels):
                true_seq = [unique_labels[label.item()] for label in label_seq if label.item() != -100]
                pred_seq = [unique_labels[pred.item()] for pred, lbl in zip(predictions[i], label_seq) if lbl.item() != -100]
                true_labels.append(true_seq)
                pred_labels.append(pred_seq)

    print(f"Validation Results for Epoch {epoch + 1}:")
    print(classification_report(true_labels, pred_labels))

# Sauvegarder le modèle et le tokenizer
output_dir = "C:/Users/adrie/OneDrive/Bureau/IPSA/Aero5/Ma513/Projet/Ma513_Project/saved_model"
os.makedirs(output_dir, exist_ok=True)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

with open(f"{output_dir}/unique_labels.json", 'w') as f:
    json.dump(unique_labels, f)

print(f"Model and tokenizer saved to {output_dir}")
